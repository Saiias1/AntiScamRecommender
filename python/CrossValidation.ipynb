{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation for Matrix Factorization\n",
    "## Robust Performance Evaluation with Statistical Significance\n",
    "\n",
    "This notebook implements 5-fold cross-validation to get robust performance estimates and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ratings data\n",
    "ratings = pd.read_csv('../data/ratings.csv')\n",
    "users = pd.read_csv('../data/users.csv')\n",
    "modules = pd.read_csv('../data/modules.csv')\n",
    "\n",
    "print(f\"Total ratings: {len(ratings)}\")\n",
    "print(f\"Total users: {len(users)}\")\n",
    "print(f\"Total modules: {len(modules)}\")\n",
    "print(f\"\\nRatings range: {ratings['rating'].min():.1f} - {ratings['rating'].max():.1f}\")\n",
    "print(f\"Rating mean: {ratings['rating'].mean():.2f}\")\n",
    "print(f\"Rating std: {ratings['rating'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Baseline Model for Cross-Validation Demo\n",
    "\n",
    "Since ML.NET training is done in C#, we'll demonstrate cross-validation with Python-based baseline models.\n",
    "\n",
    "**Models evaluated:**\n",
    "1. **User-Item Mean**: Predicts based on user average + item average - global average\n",
    "2. **Item Average**: Predicts based on module average rating\n",
    "3. **Global Mean**: Always predicts the global average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserItemMeanModel:\n",
    "    \"\"\"Baseline: user mean + item mean - global mean\"\"\"\n",
    "    def __init__(self):\n",
    "        self.global_mean = 0\n",
    "        self.user_means = {}\n",
    "        self.item_means = {}\n",
    "    \n",
    "    def fit(self, train_df):\n",
    "        self.global_mean = train_df['rating'].mean()\n",
    "        self.user_means = train_df.groupby('user_id')['rating'].mean().to_dict()\n",
    "        self.item_means = train_df.groupby('module_id')['rating'].mean().to_dict()\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        predictions = []\n",
    "        for _, row in test_df.iterrows():\n",
    "            user_mean = self.user_means.get(row['user_id'], self.global_mean)\n",
    "            item_mean = self.item_means.get(row['module_id'], self.global_mean)\n",
    "            pred = user_mean + item_mean - self.global_mean\n",
    "            predictions.append(np.clip(pred, 1, 5))  # Clip to valid range\n",
    "        return np.array(predictions)\n",
    "\n",
    "class ItemAverageModel:\n",
    "    \"\"\"Baseline: item average\"\"\"\n",
    "    def __init__(self):\n",
    "        self.global_mean = 0\n",
    "        self.item_means = {}\n",
    "    \n",
    "    def fit(self, train_df):\n",
    "        self.global_mean = train_df['rating'].mean()\n",
    "        self.item_means = train_df.groupby('module_id')['rating'].mean().to_dict()\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        predictions = []\n",
    "        for _, row in test_df.iterrows():\n",
    "            item_mean = self.item_means.get(row['module_id'], self.global_mean)\n",
    "            predictions.append(item_mean)\n",
    "        return np.array(predictions)\n",
    "\n",
    "class GlobalMeanModel:\n",
    "    \"\"\"Baseline: global mean\"\"\"\n",
    "    def __init__(self):\n",
    "        self.global_mean = 0\n",
    "    \n",
    "    def fit(self, train_df):\n",
    "        self.global_mean = train_df['rating'].mean()\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        return np.full(len(test_df), self.global_mean)\n",
    "\n",
    "print(\"✓ Baseline models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Fold Cross-Validation (k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model_class, data, k=5, model_name=\"Model\"):\n",
    "    \"\"\"Perform k-fold cross-validation\"\"\"\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Cross-Validating: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(data), 1):\n",
    "        # Split data\n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        # Train model\n",
    "        model = model_class()\n",
    "        model.fit(train_data)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(test_data)\n",
    "        actuals = test_data['rating'].values\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'train_size': len(train_data),\n",
    "            'test_size': len(test_data)\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold {fold_idx}: MAE={mae:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(fold_results)\n",
    "\n",
    "# Run cross-validation for all models\n",
    "results_user_item = cross_validate_model(UserItemMeanModel, ratings, k=5, model_name=\"User-Item Mean\")\n",
    "results_item_avg = cross_validate_model(ItemAverageModel, ratings, k=5, model_name=\"Item Average\")\n",
    "results_global = cross_validate_model(GlobalMeanModel, ratings, k=5, model_name=\"Global Mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Summary with Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(results_df, model_name):\n",
    "    \"\"\"Calculate mean, std, and 95% confidence intervals\"\"\"\n",
    "    stats_dict = {\n",
    "        'Model': model_name,\n",
    "        'MAE_mean': results_df['mae'].mean(),\n",
    "        'MAE_std': results_df['mae'].std(),\n",
    "        'MAE_CI_lower': results_df['mae'].mean() - 1.96 * results_df['mae'].std(),\n",
    "        'MAE_CI_upper': results_df['mae'].mean() + 1.96 * results_df['mae'].std(),\n",
    "        'RMSE_mean': results_df['rmse'].mean(),\n",
    "        'RMSE_std': results_df['rmse'].std(),\n",
    "        'RMSE_CI_lower': results_df['rmse'].mean() - 1.96 * results_df['rmse'].std(),\n",
    "        'RMSE_CI_upper': results_df['rmse'].mean() + 1.96 * results_df['rmse'].std(),\n",
    "        'R2_mean': results_df['r2'].mean(),\n",
    "        'R2_std': results_df['r2'].std(),\n",
    "        'R2_CI_lower': results_df['r2'].mean() - 1.96 * results_df['r2'].std(),\n",
    "        'R2_CI_upper': results_df['r2'].mean() + 1.96 * results_df['r2'].std(),\n",
    "    }\n",
    "    return stats_dict\n",
    "\n",
    "# Calculate statistics for all models\n",
    "stats_all = pd.DataFrame([\n",
    "    calculate_statistics(results_user_item, 'User-Item Mean'),\n",
    "    calculate_statistics(results_item_avg, 'Item Average'),\n",
    "    calculate_statistics(results_global, 'Global Mean')\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5-FOLD CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nMAE (Mean Absolute Error) - Lower is better:\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in stats_all.iterrows():\n",
    "    print(f\"{row['Model']:20s}: {row['MAE_mean']:.4f} ± {row['MAE_std']:.4f}  [95% CI: {row['MAE_CI_lower']:.4f} - {row['MAE_CI_upper']:.4f}]\")\n",
    "\n",
    "print(\"\\nRMSE (Root Mean Squared Error) - Lower is better:\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in stats_all.iterrows():\n",
    "    print(f\"{row['Model']:20s}: {row['RMSE_mean']:.4f} ± {row['RMSE_std']:.4f}  [95% CI: {row['RMSE_CI_lower']:.4f} - {row['RMSE_CI_upper']:.4f}]\")\n",
    "\n",
    "print(\"\\nR² (Coefficient of Determination) - Higher is better:\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in stats_all.iterrows():\n",
    "    print(f\"{row['Model']:20s}: {row['R2_mean']:.4f} ± {row['R2_std']:.4f}  [95% CI: {row['R2_CI_lower']:.4f} - {row['R2_CI_upper']:.4f}]\")\n",
    "\n",
    "# Save results\n",
    "stats_all.to_csv('../evaluation/cross_validation_results.csv', index=False)\n",
    "print(\"\\n✓ Cross-validation results saved to: evaluation/cross_validation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for each metric\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle('5-Fold Cross-Validation Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Combine all results\n",
    "all_results = pd.concat([\n",
    "    results_user_item.assign(model='User-Item Mean'),\n",
    "    results_item_avg.assign(model='Item Average'),\n",
    "    results_global.assign(model='Global Mean')\n",
    "])\n",
    "\n",
    "# MAE\n",
    "sns.boxplot(data=all_results, x='model', y='mae', ax=axes[0])\n",
    "axes[0].set_title('MAE Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# RMSE\n",
    "sns.boxplot(data=all_results, x='model', y='rmse', ax=axes[1])\n",
    "axes[1].set_title('RMSE Distribution', fontweight='bold')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# R²\n",
    "sns.boxplot(data=all_results, x='model', y='r2', ax=axes[2])\n",
    "axes[2].set_title('R² Distribution', fontweight='bold')\n",
    "axes[2].set_xlabel('')\n",
    "axes[2].set_ylabel('R²')\n",
    "axes[2].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../evaluation/plots/cross_validation_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Significance Testing\n",
    "\n",
    "Paired t-test to check if differences between models are statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (Paired t-test)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nH0: No significant difference between models\")\n",
    "print(\"H1: Significant difference exists (α = 0.05)\\n\")\n",
    "\n",
    "# User-Item Mean vs Item Average\n",
    "t_stat, p_value = stats.ttest_rel(results_user_item['mae'], results_item_avg['mae'])\n",
    "print(f\"User-Item Mean vs Item Average (MAE):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Result: {'Significant difference (reject H0)' if p_value < 0.05 else 'No significant difference'}\\n\")\n",
    "\n",
    "# User-Item Mean vs Global Mean\n",
    "t_stat, p_value = stats.ttest_rel(results_user_item['mae'], results_global['mae'])\n",
    "print(f\"User-Item Mean vs Global Mean (MAE):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Result: {'Significant difference (reject H0)' if p_value < 0.05 else 'No significant difference'}\\n\")\n",
    "\n",
    "# Item Average vs Global Mean\n",
    "t_stat, p_value = stats.ttest_rel(results_item_avg['mae'], results_global['mae'])\n",
    "print(f\"Item Average vs Global Mean (MAE):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Result: {'Significant difference (reject H0)' if p_value < 0.05 else 'No significant difference'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings\n",
    "\n",
    "### Cross-Validation Benefits:\n",
    "\n",
    "1. **Robustness**: 5-fold CV provides more reliable performance estimates than single train/test split\n",
    "2. **Variance**: Standard deviation shows model stability across different data splits\n",
    "3. **Confidence Intervals**: 95% CI gives range of expected performance in production\n",
    "4. **Statistical Significance**: t-tests confirm whether model differences are meaningful\n",
    "\n",
    "### Integration with ML.NET:\n",
    "\n",
    "For the production ML.NET Matrix Factorization model:\n",
    "1. The same cross-validation approach should be applied in C# using ML.NET's cross-validation API\n",
    "2. Expected performance: MAE ≈ 0.50 (based on previous evaluation)\n",
    "3. This would significantly outperform all Python baselines shown here\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "- ✓ Use k-fold cross-validation (k=5 or k=10) for final model evaluation\n",
    "- ✓ Report mean ± standard deviation for all metrics\n",
    "- ✓ Include confidence intervals in production monitoring\n",
    "- ✓ Perform statistical tests when comparing model versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-VALIDATION ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDeliverables:\")\n",
    "print(\"  ✓ 5-fold cross-validation results\")\n",
    "print(\"  ✓ Statistical summary with confidence intervals\")\n",
    "print(\"  ✓ Box plot visualizations\")\n",
    "print(\"  ✓ Paired t-tests for significance\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  - evaluation/cross_validation_results.csv\")\n",
    "print(\"  - evaluation/plots/cross_validation_boxplots.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
