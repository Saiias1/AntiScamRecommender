import json
nb = {
    "cells": [
        {"cell_type":"markdown","metadata":{},"source":["# Anti-Scam Recommender - Python\n","## Matrix Factorization with Surprise"]},
        {"cell_type":"markdown","metadata":{},"source":["## 1. INSTALLATION"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["# !pip install scikit-surprise pandas numpy matplotlib seaborn"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from surprise import Dataset, Reader, NormalPredictor, SVD\n","from surprise.model_selection import train_test_split, GridSearchCV, cross_validate\n","from surprise import accuracy\n","\n","sns.set_style('whitegrid')\n","plt.rcParams['figure.figsize'] = (10, 6)\n","print('Libs imported')"]},
        {"cell_type":"markdown","metadata":{},"source":["## 2. DATA LOADING"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["data_path = Path('../data/ratings.csv')\n","csharp_metrics_path = Path('../evaluation/metrics.csv')\n","output_plots_dir = Path('./plots')\n","output_plots_dir.mkdir(exist_ok=True)\n","\n","ratings_df = pd.read_csv(data_path)\n","total_ratings = len(ratings_df)\n","unique_users = ratings_df['user_id'].nunique()\n","unique_modules = ratings_df['module_id'].nunique()\n","min_rating = ratings_df['rating'].min()\n","max_rating = ratings_df['rating'].max()\n","avg_rating = ratings_df['rating'].mean()\n","\n","print(f'Total: {total_ratings:,}, Users: {unique_users:,}, Modules: {unique_modules}')\n","print(f'Range: [{min_rating:.1f}, {max_rating:.1f}], Avg: {avg_rating:.2f}')\n","\n","reader = Reader(rating_scale=(min_rating, max_rating))\n","data = Dataset.load_from_df(ratings_df[['user_id', 'module_id', 'rating']], reader)\n","ratings_df.head()"]},
        {"cell_type":"markdown","metadata":{},"source":["## 3. TRAIN/TEST SPLIT"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n","print(f'Train: {trainset.n_ratings:,}, Test: {len(testset):,}')"]},
        {"cell_type":"markdown","metadata":{},"source":["## 4. BASELINE MODEL"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["baseline_model = NormalPredictor()\n","baseline_model.fit(trainset)\n","baseline_predictions = baseline_model.test(testset)\n","baseline_mae = accuracy.mae(baseline_predictions, verbose=False)\n","baseline_rmse = accuracy.rmse(baseline_predictions, verbose=False)\n","print(f'Baseline - MAE: {baseline_mae:.4f}, RMSE: {baseline_rmse:.4f}')"]},
        {"cell_type":"markdown","metadata":{},"source":["## 5. SVD MODEL"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["svd_params = {'n_factors': 100, 'n_epochs': 20, 'lr_all': 0.1, 'random_state': 42}\n","svd_model = SVD(**svd_params)\n","svd_model.fit(trainset)\n","svd_predictions = svd_model.test(testset)\n","svd_mae = accuracy.mae(svd_predictions, verbose=False)\n","svd_rmse = accuracy.rmse(svd_predictions, verbose=False)\n","print(f'SVD - MAE: {svd_mae:.4f}, RMSE: {svd_rmse:.4f}')"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["def precision_at_k(predictions, k=5, threshold=3.5):\n","    user_preds = {}\n","    for uid, iid, true_r, est, _ in predictions:\n","        if uid not in user_preds:\n","            user_preds[uid] = []\n","        user_preds[uid].append((iid, true_r, est))\n","    precisions = []\n","    for uid, ratings in user_preds.items():\n","        ratings.sort(key=lambda x: x[2], reverse=True)\n","        top_k = ratings[:k]\n","        n_relevant = sum(1 for _, true_r, _ in top_k if true_r >= threshold)\n","        precisions.append(n_relevant / k)\n","    return np.mean(precisions)\n","\n","svd_p5 = precision_at_k(svd_predictions)\n","print(f'Precision@5: {svd_p5:.4f}')"]},
        {"cell_type":"markdown","metadata":{},"source":["## 6. HYPERPARAMETER TUNING"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["param_grid = {'n_factors': [50, 100, 150], 'n_epochs': [10, 20, 40], 'lr_all': [0.01, 0.05, 0.1]}\n","gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3, n_jobs=-1)\n","gs.fit(data)\n","print(f\"Best RMSE: {gs.best_score['rmse']:.4f}, Best params: {gs.best_params['rmse']}\")\n","results_df = pd.DataFrame(gs.cv_results).sort_values('mean_test_rmse')\n","results_df[['params', 'mean_test_rmse', 'mean_test_mae']].head()"]},
        {"cell_type":"markdown","metadata":{},"source":["## 7. CROSS-VALIDATION"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["cv_results = cross_validate(SVD(**svd_params), data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n","cv_mae_mean, cv_mae_std = np.mean(cv_results['test_mae']), np.std(cv_results['test_mae'])\n","cv_rmse_mean, cv_rmse_std = np.mean(cv_results['test_rmse']), np.std(cv_results['test_rmse'])\n","print(f'CV MAE: {cv_mae_mean:.4f} (±{cv_mae_std:.4f}), RMSE: {cv_rmse_mean:.4f} (±{cv_rmse_std:.4f})')"]},
        {"cell_type":"markdown","metadata":{},"source":["## 8. COMPARISON WITH C#"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["try:\n","    csharp_df = pd.read_csv(csharp_metrics_path)\n","    comp_data = {'Model': [], 'Language': [], 'MAE': [], 'RMSE': []}\n","    for _, row in csharp_df.iterrows():\n","        comp_data['Model'].append(row['ModelName'])\n","        comp_data['Language'].append('C#')\n","        comp_data['MAE'].append(row['MAE'])\n","        comp_data['RMSE'].append(row['RMSE'])\n","    comp_data['Model'].extend(['SVD', 'NormalPredictor'])\n","    comp_data['Language'].extend(['Python', 'Python'])\n","    comp_data['MAE'].extend([svd_mae, baseline_mae])\n","    comp_data['RMSE'].extend([svd_rmse, baseline_rmse])\n","    comparison_df = pd.DataFrame(comp_data)\n","    display(comparison_df)\n","    csharp_mf = csharp_df[csharp_df['ModelName'] == 'Matrix Factorization'].iloc[0]\n","    mae_diff = ((svd_mae - csharp_mf['MAE']) / csharp_mf['MAE']) * 100\n","    print(f'Python vs C# MAE diff: {mae_diff:+.2f}%')\n","except:\n","    print('C# metrics not found')\n","    comparison_df = None"]},
        {"cell_type":"markdown","metadata":{},"source":["## 9. VISUALIZATIONS"]},
        {"cell_type":"code","execution_count":None,"metadata":{},"outputs":[],"source":["if comparison_df is not None:\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","    ml = comparison_df[comparison_df['Model'].isin(['SVD', 'Matrix Factorization'])].copy()\n","    ml['Label'] = ml['Model'] + ' (' + ml['Language'] + ')'\n","    ax.bar(ml['Label'], ml['MAE'], color=['#3498db', '#e74c3c'], alpha=0.7)\n","    ax.set_ylabel('MAE', fontweight='bold')\n","    ax.set_title('MAE: Python vs C#', fontweight='bold')\n","    ax.grid(axis='y', alpha=0.3)\n","    plt.savefig(output_plots_dir / 
